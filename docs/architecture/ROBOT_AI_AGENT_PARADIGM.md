# The Robot AI Agent Paradigm

**The LLM is the robot's brain, not its assistant.**

---

## Core Idea

Traditional robotics places intelligence in compiled firmware: hardcoded state machines, PID controllers, and hand-tuned heuristics. The robot's behavior is fixed at compile time. To change what the robot does, a human must rewrite code, recompile, and reflash.

LLMos inverts this. The LLM is not a tool that helps a human write robot firmware. The LLM *is* the robot's cognitive layer. It receives sensor data, reasons about a world model, selects subgoals, and emits structured instructions -- every cycle, in closed loop, at runtime.

The microcontroller becomes a peripheral. It exposes sensors as readable tools and actuators as callable tools. All behavioral intelligence lives in the LLM.

```
Traditional:  Human -> Code -> Compile -> Flash -> Fixed Behavior
LLMos:        Sensors -> World Model -> LLM -> Structured Decision -> Classical Planner -> Actuators
```

---

## Inversion of Control

In classical robotics, the firmware is the authority. It reads sensors, runs control loops, and drives motors. The LLM, if present, is called occasionally for advice.

In LLMos, the LLM is the authority. Every cycle:

1. Sensors produce readings (distance, camera, IMU)
2. The world model is updated from sensor data
3. The world model is serialized (RLE JSON, top-down image, or patch)
4. Candidate subgoals are generated by classical heuristics
5. The LLM receives the full context and picks a strategy
6. A classical A* planner computes a collision-free path
7. The HAL translates the path into motor commands
8. The firmware executes the commands and reports results

The LLM decides WHERE to go. Classical planners decide HOW to get there. The firmware handles the physical execution. Each layer has a strict contract with the next.

```
LLM Layer:       Strategy selection (MOVE_TO, EXPLORE, ROTATE, STOP)
Planner Layer:   A* pathfinding on occupancy grid
HAL Layer:       Motor commands with safety validation
Firmware Layer:  Deterministic execution with fallback logic
```

This separation is the safety architecture. Even if the LLM makes a questionable choice, the planner enforces collision-free paths, the HAL enforces motor limits, and the firmware enforces deterministic fallback when host communication fails.

---

## Dual-LLM Design

LLMos uses two different LLMs for two fundamentally different jobs:

### Claude Opus 4.6 -- The Development Brain

Runs on the host machine. Handles everything that requires deep reasoning, large context windows, and multi-step planning:

- Creating and evolving agent definitions (markdown files)
- Orchestrating multi-agent workflows
- Generating and promoting skills across volumes
- Consolidating execution memory
- Proposing kernel improvements
- Debugging and architectural reasoning

This LLM operates at human timescales (seconds to minutes). Latency does not matter. Reasoning depth does.

### Qwen3-VL-8B -- The Runtime Brain

Runs locally on host GPU (8GB+ VRAM) or via OpenRouter. Handles real-time perception and decision-making:

- **Instinct mode**: Single-pass VLM inference (~200-500ms). Reactive behaviors: obstacle avoidance, wall following, object tracking. The reflex arc.
- **Planner mode**: RSA-enhanced deeper reasoning (3-8s). Strategy: exploration planning, recovery from novel situations, skill generation. The prefrontal cortex.
- **Vision**: Unified vision-language model. Sees camera frames, estimates depth, identifies objects, reads text -- all in one multimodal pass.

The DualBrainController (`lib/runtime/dual-brain-controller.ts`) manages escalation between instinct and planner based on:

| Trigger | Brain | Reason |
|---|---|---|
| Imminent collision | Instinct | Time-critical, reflexive |
| Open corridor | Instinct | Simple, well-understood |
| Unknown object | Planner | Requires analysis |
| Stuck for N cycles | Planner | Needs new strategy |
| Multi-step goal | Planner | Requires planning |
| Fleet coordination | Planner | Cross-robot reasoning |

Neither brain blocks the other. The development LLM can reason about architecture while the runtime LLM maintains closed-loop control.

---

## The World Model as Cognitive Map

An intelligent robot must build an internal model of its world. The WorldModel class (`lib/runtime/world-model.ts`) maintains a 50x50 occupancy grid (10cm resolution, 5m x 5m arena) that represents the robot's understanding:

### Cell States

| State | Symbol | Meaning |
|---|---|---|
| `unknown` | U | Not yet observed |
| `free` | F | Safe to traverse |
| `explored` | E | Robot has been here |
| `obstacle` | O | Contains an obstacle |
| `wall` | W | Arena boundary |
| `collectible` | C | Contains a collectible |
| `collected` | D | Collectible was here, now collected |
| `path` | P | Part of a planned path |

### Grid Properties

Each cell tracks:
- **State**: One of the above
- **Confidence** (0.0-1.0): How certain the robot is about this cell
- **Last updated**: Timestamp for temporal decay
- **Visit count**: How many times the robot has been in this cell
- **Distance reading**: Last sensor measurement at this cell

### Three Serialization Formats

The world model is serialized for the LLM in three formats, each serving a different purpose:

1. **Format A -- RLE JSON** (`world-model-serializer.ts`): Compact run-length encoded grid for structured reasoning. Primary format. Example: `"U:1200,F:800,O:200,W:300"`
2. **Format B -- Top-down image** (`map-renderer.ts`): 500x500 pixel color-coded image for multimodal VLM input. The LLM sees the map visually alongside the camera frame.
3. **Format C -- ASCII grid** (`world-model-serializer.ts`): 25x25 downsampled text grid for debugging and lightweight contexts.
4. **Delta patches**: Only cells changed since last serialization, for subsequent cycles.

### Predictive Layer

The PredictiveWorldModel (`lib/runtime/predictive-world-model.ts`) extrapolates what might be in unknown cells based on observed patterns:

- **Wall continuation**: If a wall is observed, predict it continues in the same direction
- **Corridor detection**: If walls form a corridor, predict the corridor continues
- **Open space expansion**: If a large open area is seen, predict it extends
- **Symmetry**: Mirror known structure when symmetry is detected

Predictions are marked with low confidence (0.2-0.3) and automatically verified when the robot observes those cells. Correct predictions boost model confidence; incorrect predictions trigger re-evaluation.

---

## The Execution Frame

The execution frame (`lib/runtime/execution-frame.ts`) is the atomic unit of LLMos computation. Every cycle, the runtime LLM receives a frame and emits the next one.

An execution frame contains:

| Element | Description |
|---|---|
| **Goal** | What the agent is trying to achieve (text) |
| **History** | Last N cycles providing temporal context |
| **Internal State** | Variables representing the agent's beliefs |
| **World Model** | RLE-encoded occupancy grid + symbolic layer + candidates |
| **Sensor Inputs** | Current physical readings (distance, camera, IMU) |
| **Previous Action Results** | Outcomes from last cycle (success, blocked, timeout, collision) |
| **Fallback Logic** | Deterministic behavior that runs if LLM inference fails |

The frame is designed so that the LLM can reason about the complete context in a single pass, without needing memory beyond the frame itself.

---

## The Scene Graph as Semantic Understanding

The occupancy grid tells the robot *where things are*. The scene graph (`lib/runtime/scene-graph/`) tells it *what things are*.

The scene graph maintains:
- **Typed objects**: wall, obstacle, beacon, collectible, robot -- each with position, bounding box, confidence
- **Topology**: Waypoint graph with edges (clear, blocked, unknown) for high-level navigation reasoning
- **Semantic queries**: Natural language queries against the graph (e.g., "nearest obstacle ahead")

The VisionSceneBridge (`lib/runtime/vision-scene-bridge.ts`) projects camera detections into world coordinates and registers them as scene graph nodes with deduplication (spatial threshold of 0.3m).

---

## HAL: Simulation and Physical Duality

The Hardware Abstraction Layer (`lib/hal/types.ts`) provides a unified interface that works identically in both simulation and physical environments:

### HAL Modes

| Mode | Description |
|---|---|
| `simulation` | Three.js renderer with simulated physics |
| `physical` | ESP32-S3 hardware via serial/WiFi |
| `hybrid` | Physical robot with simulation overlay |

### HAL Interfaces

| Interface | Key Methods |
|---|---|
| **Locomotion** | `drive(left, right)`, `moveTo(x, y, z)`, `rotate(dir, deg)`, `moveForward(cm)`, `stop()`, `getPose()` |
| **Vision** | `captureFrame()`, `scan(mode)`, `getDistanceSensors()`, `getLineSensors()`, `getIMU()` |
| **Manipulation** | `moveTo(x, y, z)`, `grasp(force)`, `extend(distance)`, `retract()`, `setPrecisionMode(enabled)` |
| **Communication** | `speak(text)`, `playSound(id)`, `setLED(color)`, `listenForCommand()` |
| **Safety** | `getStatus()`, `emergencyStop()`, `setOperatingLimits(limits)` |

The same navigation decision from the LLM produces the same robot behavior whether the target is a simulated Three.js scene or a physical ESP32 in a real arena.

---

## Vision Pipeline

The vision pipeline converts camera frames into world model updates:

```
Camera Frame
  -> VLM (Qwen3-VL-8B) or GroundTruthVisionSimulator
  -> VisionFrame (detections + scene analysis)
  -> VisionWorldModelBridge (sensor-bridge.ts)
    -> Project detections onto occupancy grid
    -> Mark free cells in open directions
    -> Mark obstacle cells at detection positions
    -> Compute frontier cells
  -> VisionSceneBridge (vision-scene-bridge.ts)
    -> Register detections as SceneGraph nodes
    -> Deduplicate against existing nodes
  -> World model now updated for next LLM cycle
```

Two bridge modes exist:

- **Ground-truth bridge** (`world-model-bridge.ts`): Rasterizes the full arena (walls, obstacles, beacons) onto the grid from simulation state. Used for development and testing.
- **Vision bridge** (`sensor-bridge.ts`): Builds the grid incrementally from VLM camera output alone. Grid starts fully unknown, explored area grows as the robot moves. Limited to 60-degree camera FOV per frame.

Both produce identical serialized output. The LLM does not know which bridge is active.

---

## Multi-Robot Fleet Coordination

The FleetCoordinator (`lib/runtime/fleet-coordinator.ts`) manages multiple robots in the same arena:

### Shared World Model Merging

Each robot maintains its own WorldModel (indexed by device ID). The coordinator periodically merges them into a shared view using configurable strategies:
- **max_confidence**: Keeps the highest-confidence value for each cell
- **latest_update**: Keeps the most recently updated value

### Frontier Task Assignment

The coordinator identifies frontier cells (boundaries between explored and unknown) across all robots and assigns exploration tasks to minimize overlap. Tasks are typed:
- `explore_frontier`: Navigate to an unexplored boundary
- `navigate_to`: Go to a specific coordinate
- `patrol`: Patrol a region
- `idle`: No task

### Conflict Resolution

A minimum target separation distance (default 0.5m) prevents multiple robots from claiming the same frontier. Tasks are reassigned periodically (default every 5s).

---

## The Candidate System

The LLM never picks raw coordinates. Each cycle, the CandidateGenerator (`lib/runtime/candidate-generator.ts`) produces 3-5 ranked subgoals that the classical planner has already validated as reachable:

| Candidate Type | Description |
|---|---|
| `subgoal` | Point along the path toward the goal |
| `frontier` | Boundary between explored and unknown (for exploration) |
| `recovery` | Safe retreat position when stuck |
| `waypoint` | Named waypoint from the topology graph |

Each candidate has a heuristic score computed from:
- Distance to goal (closer = higher)
- Clearance from obstacles (more clearance = higher)
- Novelty (more unexplored cells nearby = higher)
- Feasibility (lower path cost = higher)

The LLM receives candidates formatted as a ranked list with scores and explanatory notes, then selects one by ID. This constrains the LLM's output space while preserving strategic flexibility.

---

## LLM Decision Schema

The LLM responds with a structured JSON decision:

```json
{
  "action": {
    "type": "MOVE_TO",
    "target_id": "c1",
    "target_pos_m": [1.5, 2.0],
    "speed": "normal"
  },
  "fallback": {
    "if_failed": "ROTATE",
    "direction": "left",
    "degrees": 45
  },
  "explanation": "Moving toward frontier c1 to explore unknown region northeast.",
  "world_model_update": {
    "corrections": [
      { "gx": 25, "gy": 30, "state": "free", "confidence": 0.7 }
    ]
  }
}
```

Action types: `MOVE_TO`, `EXPLORE`, `ROTATE`, `STOP`, `RECOVER`.

The `parseNavigationDecision()` function in `navigation-types.ts` extracts JSON from the LLM's response with robust fallback handling for malformed output.

---

## Summary

The LLMos robot AI agent paradigm is built on five principles:

1. **Inversion of control**: The LLM decides, classical systems execute
2. **Dual-LLM separation**: Development brain (deep, slow) vs runtime brain (reactive, fast)
3. **World model as interface**: The LLM sees a structured representation, not raw sensors
4. **Safety through layering**: LLM -> planner -> HAL -> firmware, each with validation
5. **Simulation-physical duality**: Same code, same decisions, different hardware backends
